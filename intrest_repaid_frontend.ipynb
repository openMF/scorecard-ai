{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Using the temp file previously created (to avoid re-downloading the data)\n",
    "df_cleaned = pd.read_csv('temp.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specificaly for the frontent part we havent done encoding of the categorical columns\n",
    "# to make it easy for the user to enter the data\n",
    "\n",
    "date_columns = [\n",
    "    'activation_date',\n",
    "    'date_of_birth',\n",
    "    'approvedon_date',\n",
    "    'expected_disbursedon_date',\n",
    "    'disbursedon_date',\n",
    "    'expected_maturedon_date',\n",
    "    'maturedon_date',\n",
    "    'transaction_date',\n",
    "    'submitted_on_date',\n",
    "    'validatedon_date',\n",
    "    'created_date',\n",
    "]\n",
    "\n",
    "numerical_columns = [\n",
    "    'gender_cv_id',\n",
    "    'legal_form_enum',\n",
    "    'interest_period_frequency_enum',\n",
    "    'interest_method_enum',\n",
    "    'interest_calculated_in_period_enum',\n",
    "    'transaction_type_enum',\n",
    "    'principal_amount',\n",
    "    'nominal_interest_rate_per_period',\n",
    "    'annual_nominal_interest_rate',\n",
    "    'amount',\n",
    "    'term_frequency',\n",
    "    'number_of_repayments',\n",
    "    'principal_disbursed_derived',\n",
    "    'total_expected_repayment_derived',\n",
    "    'total_expected_costofloan_derived',\n",
    "    'principal_portion_derived',\n",
    "    'outstanding_loan_balance_derived'\n",
    "    'principal_repaid_derived',\n",
    "    'principal_outstanding_derived',\n",
    "    'interest_charged_derived',\n",
    "    'interest_repaid_derived',\n",
    "    'interest_outstanding_derived',\n",
    "    'total_repayment_derived',\n",
    "    'total_costofloan_derived',\n",
    "    'total_outstanding_derived',\n",
    "    \n",
    "]\n",
    "other_columns_not_encoded = [\n",
    "    'office_joining_date',\n",
    "    'has_email_address',\n",
    "    'status_enum',\n",
    "    'has_mobile_no',\n",
    "    'validatedon_userid',\n",
    "    'loan_transaction_strategy_id',\n",
    "    'is_reversed',\n",
    "    'submittedon_date_client',\n",
    "    'submittedon_date_loan',\n",
    "    'principal_amount_proposed',   \n",
    "    'manually_adjusted_or_reversed'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns are redandant and can be dropped\n",
    "df_cleaned = df_cleaned.drop(columns=other_columns_not_encoded)\n",
    "\n",
    "for col in date_columns:\n",
    "    df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "\n",
    "# Find the minimum date\n",
    "min_date = df_cleaned[date_columns].min().min()\n",
    "reference_date = pd.to_datetime(min_date)\n",
    "\n",
    "# Conversion to days usign the reference date\n",
    "for col in date_columns:\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(reference_date)\n",
    "    df_cleaned[col] = (df_cleaned[col] - reference_date).dt.days\n",
    "\n",
    "df_encoded = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_encoded.isnull().mean() * 100\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "print(f\"Number of columns with missing values: {len(missing_values)}\")\n",
    "print(missing_values.index)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns=missing_values.index)\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the correlation matrix to find the most important features\n",
    "# we will drop the columns that are not correlated with the target variable and keep the ones that are not too highly correlated with each other\n",
    "\n",
    "correlation_matrix = df_encoded.corr()\n",
    "target_variable = 'interest_repaid_derived'\n",
    "target_correlation = correlation_matrix[target_variable].sort_values(ascending=False)\n",
    "\n",
    "# The top 10 most correlated features and the least 10 correlated features\n",
    "print(target_correlation[:10])\n",
    "print(\"\\n\")\n",
    "print(target_correlation[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now given we are predicting the interest_repaid_derived we wont be having the following columns\n",
    "# Also these columns are highly correlated with the target variable (which is understandable)\n",
    "highly_corr_future = [\n",
    "    'principal_disbursed_derived',\n",
    "    'total_expected_repayment_derived',\n",
    "    'total_expected_costofloan_derived',\n",
    "    'outstanding_loan_balance_derived',\n",
    "    'principal_repaid_derived',\n",
    "    'principal_outstanding_derived',\n",
    "    'interest_charged_derived',\n",
    "    'interest_outstanding_derived',\n",
    "    'total_repayment_derived',\n",
    "    'total_costofloan_derived',\n",
    "    'total_outstanding_derived',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(columns=highly_corr_future)\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = ['gender_cv_id_16', 'gender_cv_id_17', 'gender_cv_id_750143','legal_form_enum_1', 'legal_form_enum_2', 'has_email_address_0','has_email_address_1', 'interest_period_frequency_enum_2','interest_period_frequency_enum_3', 'interest_method_enum_0','interest_method_enum_1', 'interest_calculated_in_period_enum_0','interest_calculated_in_period_enum_1','transaction_type_enum_1', 'transaction_type_enum_2']\n",
    "numerical_columns = df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop('interest_repaid_derived', axis=1)\n",
    "y = df_encoded['interest_repaid_derived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are currently using mean to fill the missing values when the user doest input the data\n",
    "print(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We divide the numbers into bins, this makes it more generalised\n",
    "# this makes sense as 3 percent and 3.1 percent are not that different\n",
    "\n",
    "num_quantiles = 16\n",
    "\n",
    "# Apply quantile-based binning\n",
    "y_binned, bin_edges = pd.qcut(y, q=num_quantiles, labels=False, retbins=True, duplicates='drop')\n",
    "\n",
    "mask = ~y_binned.isna()\n",
    "\n",
    "# Apply the mask to both X and y_binned to drop corresponding rows\n",
    "X_filtered = X[mask]\n",
    "y_binned_filtered = y_binned[mask]\n",
    "\n",
    "# Check the distribution of the binned values\n",
    "print(y_binned.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ranges = [(bin_edges[i], bin_edges[i+1]) for i in range(len(bin_edges)-1)]\n",
    "for i, bin_range in enumerate(bin_ranges):\n",
    "    print(f\"Bin {i}: {bin_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_binned_filtered, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the dataset is large we will be using a random sample of the data\n",
    "X_random = X_train.sample(frac=0.1, random_state=42)\n",
    "y_random = y_train[X_random.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first row of X_random\n",
    "print(X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.iloc[0].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=100, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_random, y_random)\n",
    "\n",
    "# Predict the target values\n",
    "y_pred_train = logistic_model.predict(X_random)\n",
    "y_pred_test = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "train_accuracy = accuracy_score(y_random, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import pickle\n",
    "with open('logistic_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(logistic_model, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "decision_tree_model.fit(X_random, y_random)\n",
    "\n",
    "# Predict the target values\n",
    "y_pred_train = decision_tree_model.predict(X_random)\n",
    "y_pred_test = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "train_accuracy = accuracy_score(y_random, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('decision_tree_model_low.pkl', 'wb') as model_file:\n",
    "    pickle.dump(decision_tree_model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_random, y_random)\n",
    "\n",
    "# Predict the target values\n",
    "y_pred_train = xgb_model.predict(X_random)\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "train_accuracy = accuracy_score(y_random, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(xgb_model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
